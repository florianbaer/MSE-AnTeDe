{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "official-furniture",
      "metadata": {
        "id": "official-furniture"
      },
      "source": [
        "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
        "\n",
        "# AnTeDe Lab 11: Question Answering using BERT\n",
        "\n",
        "by Andrei Popescu-Belis (HES-SO)\n",
        "using the [ðŸ¤— Huggingface models](https://huggingface.co/models),\n",
        "an [article by Marius Borcan](https://programmerbackpack.com/bert-nlp-using-distilbert-to-build-a-question-answering-system/) and \n",
        "an [article by Ramsi Goutham](https://towardsdatascience.com/simple-and-fast-question-answering-system-using-huggingface-distilbert-single-batch-inference-bcf5a5749571)\n",
        "\n",
        "**Summary**\n",
        "The goal of this lab is to implement and test a simple question answering (QA) system over a set of articles.  The structure of the lab is as follows:\n",
        "1. Answer extraction from a text fragment -- in this part, you will use a pre-trained model named DistilBERT (a lighter version of BERT) which can extract the most likely answer to a given question from a text fragment (in English).\n",
        "2. Text retrieval given a question -- in this part, you will reuse code from Lab 4 (Search Engine) to design a paragraph retrieval system over the 300-article Lee corpus provided with `gensim`. \n",
        "3. Integration and testing -- in this part, you will put together the functions from the previous two parts, and test your system end-to-end by designing a test set of 10 questions.\n",
        "\n",
        "## Implemented by:\n",
        "- Adrian Willi (adrian.willi@hslu.ch)\n",
        "- Florian BÃ¤r (florian.baer@hslu.ch)\n",
        "\n",
        "<font color='green'>Please answer the questions in green within this notebook.  The expected answers are generally very short: 1-2 commands or 2-3 lines of explanations.  At the end, please submit the completed notebook under the corresponding homework on Moodle.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "material-appreciation",
      "metadata": {
        "id": "material-appreciation"
      },
      "source": [
        "## 1. Answer extraction using DistilBERT\n",
        "\n",
        "As you know, the BERT pre-trained model can be fine-tuned for question answering, by training it to provide the start and end word of an input text fragment which is most likely the answer to an input question.  You will use the ðŸ¤— Huggingface Python module called `transformers`, and later use a DistilBERT model also provided by ðŸ¤— Huggingface.\n",
        "\n",
        "### a. Install `pytorch` and `transformers`\n",
        "\n",
        "Use the instructions provided by [PyTorch](https://pytorch.org/get-started/locally/#start-locally) and by [Huggingface](https://github.com/huggingface/transformers#installation).  The use of `conda` is recommended."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers -q"
      ],
      "metadata": {
        "id": "5Lk0BqGaiOHD"
      },
      "id": "5Lk0BqGaiOHD",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "environmental-shadow",
      "metadata": {
        "id": "environmental-shadow"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleared-geology",
      "metadata": {
        "id": "cleared-geology"
      },
      "source": [
        "<font color='green'>Please generate a random 2x2x2 tensor with Pytorch.  Please display whether the workstation you use has a GPU or not.</font><br/>\n",
        "(Note: a GPU is not required for this lab.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "secret-fever",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "secret-fever",
        "outputId": "eb4381c1-68cd-40bb-ab5c-c6c069544fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.8682, 0.7508],\n",
            "         [0.3882, 0.1904]],\n",
            "\n",
            "        [[0.3288, 0.0716],\n",
            "         [0.4065, 0.5929]]])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.rand((2,2,2)))\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "resistant-multimedia",
      "metadata": {
        "id": "resistant-multimedia"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wicked-junior",
      "metadata": {
        "id": "wicked-junior"
      },
      "source": [
        "ðŸ¤— Huggingface provides a very large repository of Transformer-based models at https://huggingface.co/models.\n",
        "\n",
        "<font color='green'>Please use the search interface (in a browser) and find out *how many models containing the name 'distilbert' for Question Answering* are available.  If we exclude those submitted by individual users, how many models are there left?  Please paste below their name and version date, and the size of their 'pytorch_model.bin' file.</font>\n",
        "\n",
        "\n",
        "<font color='green'>By looking at their \"model cards\", which model has the highest performance on the SQuAD dev set?</font>  In what follows, we will use this model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Totally, there are 203 Models containing the name 'distilbert' for question answering. \n",
        "\n",
        "Excluding those submitted by users there are 2 models.\n",
        "\n",
        "- **distilbert-base-uncased-distilled-squad** with a size of 253 MB\n",
        "  - F1 score of 86.9\n",
        "- **distilbert-base-cased-distilled-squad** with a size of 249 MB\n",
        "  - F1 score of 87.1"
      ],
      "metadata": {
        "id": "uEz7xCDtkbsJ"
      },
      "id": "uEz7xCDtkbsJ"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "functioning-volume",
      "metadata": {
        "id": "functioning-volume"
      },
      "outputs": [],
      "source": [
        " model_name = 'distilbert-base-cased-distilled-squad'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-surveillance",
      "metadata": {
        "id": "saved-surveillance"
      },
      "source": [
        "### b. Tokenization of the input\n",
        "\n",
        "We will use here a tokenizer called `DistilBertTokenizer` to tokenize the question and the text fragment and transform the numbers into numerical indices.  The documentation for this tokenizer is included in the general documentation of DistilBERT models at: https://huggingface.co/transformers/model_doc/distilbert.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "muslim-police",
      "metadata": {
        "id": "muslim-police"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, AutoTokenizer\n",
        "# you could use the AutoTokenizer as well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-extreme",
      "metadata": {
        "id": "ordered-extreme"
      },
      "source": [
        "<font color='green'>Please create an instance of such a tokenizer \n",
        "using the pre-trained model named 'distilbert-base-cased'.  The command\n",
        "will download the necessary model the first time you use it.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "united-bargain",
      "metadata": {
        "id": "united-bargain"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comprehensive-wichita",
      "metadata": {
        "id": "comprehensive-wichita"
      },
      "source": [
        "<font color='green'>What does this instance return if you **call** it with a sentence (a *string*) as an argument?  Please write the instruction below, and be sure you include the word 'Winterthur' in your sentence.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "prompt-depression",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prompt-depression",
        "outputId": "ac257dcf-f0ed-49b3-c618-a2d98982d7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "{'input_ids': [[101, 146, 1821, 1177, 6276, 1112, 178, 1821, 5497, 170, 173, 19593, 2511, 1107, 4591, 1582, 2149, 102], [101, 146, 1177, 1112, 178, 1821, 5497, 3489, 1107, 4591, 1582, 2149, 102, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "#print(tokenizer.tokenize('I am so funny as i am eating a dÃ¶ner in Winterthur'))\n",
        "#print(tokenizer.tokenize('I so as i am eating a dÃ¶ner in Winterthur'))\n",
        "sentences = ['I am so funny as i am eating a dÃ¶ner in Winterthur',\n",
        "             'I so as i am eating fish in Winterthur']\n",
        "tokens = tokenizer(sentences, padding=True)\n",
        "print(tokenizer(sentences[0]).keys())\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "maritime-agriculture",
      "metadata": {
        "id": "maritime-agriculture"
      },
      "source": [
        "<font color='green'>Please explain in your own words the meaning of the two components of the output above.  For that, please use the [documentation of the class DistilBertTokenizer](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer), and be sure you read the documentation of its *superclasses* as well.  Under what superclass do you find the links to the [glossary entries](https://huggingface.co/transformers/glossary.html) that best explain the two components, and what are these entries?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer returns as seen in the documentation the input ids and the attention_mask. The input_ids of the tokens are the ids fed into the model. These ids are used to identify a token. The attention mask is used to verify the length of the input sequece. This makes it possible to scale the input size fo the text."
      ],
      "metadata": {
        "id": "pR1wgFQIBVdt"
      },
      "id": "pR1wgFQIBVdt"
    },
    {
      "cell_type": "markdown",
      "id": "separate-chocolate",
      "metadata": {
        "id": "separate-chocolate"
      },
      "source": [
        "<font color='green'>If you haven't explained above, please explain here the cause of the difference between the number of words of your sentence, and the number of tokens in the observed output.  Please display the tokens of the output. You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentence is not split into words, but instead splitted into subword tokens, which often are used to represent less common words.This approach is similar as it is done in fasttext."
      ],
      "metadata": {
        "id": "ID7pAx3QziEX"
      },
      "id": "ID7pAx3QziEX"
    },
    {
      "cell_type": "markdown",
      "id": "competent-preparation",
      "metadata": {
        "id": "competent-preparation"
      },
      "source": [
        "<font color='green'>How can you convert back the first part of the output to the original string?\n",
        "Please write and execute the command(s) below.  You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "flying-courtesy",
      "metadata": {
        "id": "flying-courtesy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ec1694-a516-453a-91be-c0070f5e309d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] I am so funny as i am eating a dÃ¶ner in Winterthur [SEP]\n",
            "[CLS] I so as i am eating fish in Winterthur [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokens.input_ids[0]))\n",
        "print(tokenizer.decode(tokens.input_ids[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nuclear-auditor",
      "metadata": {
        "id": "nuclear-auditor"
      },
      "source": [
        "### c. Generation of input in the desired form\n",
        "\n",
        "We need to generate input in the form expected by the `DistilBertForQuestionAnswering` class.  This means providing the question, the text from which the answer must be extracted, with the proper [CLS] and [SEP] tokens, and the attention masks.  Moreover, using DistilBERT requires that the lists of indices returned by the tokenizer are Pytorch tensors (see tokenizer's option `return_tensors`).\n",
        "\n",
        "<font color='green'>What is the correct way to call the tokenizer in order to obtain these results?  You can use the example provided at the end of the [DistilBertForQuestionAnswering](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) documentation.  <br/>Please define a *question* and a *text* string of your own, and store the result of the tokenizer in a variable called *input*.  <br/>   Please verify (by converting back to the result) that the input has the correct tokens.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "noticed-firmware",
      "metadata": {
        "id": "noticed-firmware",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b619c693-2015-4ebd-d3a6-03345daa316a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] What is your age? [SEP] As seen in the internet, the age of you is 25 Years old, which is very old. [SEP]\n",
            "28\n"
          ]
        }
      ],
      "source": [
        "question = 'What is your age?'\n",
        "answer = 'As seen in the internet, the age of you is 25 Years old, which is very old.'\n",
        "inputs = tokenizer(question, answer, return_tensors='pt')\n",
        "print(tokenizer.decode(inputs.input_ids[0]))\n",
        "print(inputs.input_ids[0].shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hindu-ridge",
      "metadata": {
        "id": "hindu-ridge"
      },
      "source": [
        "### d. Execution of the model over the input question and text\n",
        "\n",
        "In this section, you will create an instance of the BERT neural network adapted to question answering.  The class is named `DistilBertForQuestionAnswering`.  The model itself (the weights) is the one that you found at the end of (1a) above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "alien-consortium",
      "metadata": {
        "id": "alien-consortium"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sporting-pricing",
      "metadata": {
        "id": "sporting-pricing"
      },
      "source": [
        "<font color='green'>Please create an instance of the model here.</font>  The data will be downloaded the first time you create it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "fatty-disorder",
      "metadata": {
        "id": "fatty-disorder"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "packed-introduction",
      "metadata": {
        "id": "packed-introduction"
      },
      "source": [
        "The results of applying the model to your question and text (i.e. extracting the answer) are obtained by calling the model with the correct inputs.  \n",
        "\n",
        "<font color='green'>Please use the inputs you obtained above and read the [documentation of the DistilBertForQuestionAnswering class](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) (under *forward*) to apply the model to your data.  Store the results in a variable called *outputs*.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "southeast-murder",
      "metadata": {
        "id": "southeast-murder"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-rhythm",
      "metadata": {
        "id": "saved-rhythm"
      },
      "source": [
        "<font color='green'>Where are the probability values for the position of the **start** of the answer in *outputs*?</font> \n",
        "- The value of the logit is 9.8292.\n",
        "\n",
        "<font color='green'>Are these actual probabilities or other type of coefficients?</font> \n",
        "- These are not probabilities. Else they would sum up to 1 or 100% - what they don't do. They are the logits before SoftMax was applied.\n",
        "\n",
        "<font color='green'>\n",
        "How many values are there, and is this coherent with your observations in (1b)</font> \n",
        "\n",
        "- There is for each token given into the system a value as a logit to return the start and the end token as a response for the given question which was passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "id": "robust-monkey",
      "metadata": {
        "id": "robust-monkey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c829ef-d4e7-4000-83ae-99d8b6215192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9.8292)\n",
            "Length of start_logits is torch.Size([1, 28])\n",
            "Index is from 18\n",
            "Index is to 20\n",
            "tensor(8.7799)\n",
            "2\n",
            "25 Years old\n"
          ]
        }
      ],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "print(outputs.start_logits.max())\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "print(f'Length of start_logits is {(outputs.start_logits.shape)}')\n",
        "print(f'Index is from {outputs.start_logits.argmax()}')\n",
        "print(f'Index is to {outputs.end_logits.argmax()}')\n",
        "print(outputs.end_logits.max())\n",
        "print(len(outputs))\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "print(tokenizer.decode(predict_answer_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scenic-custom",
      "metadata": {
        "id": "scenic-custom"
      },
      "source": [
        "### e. Determination of the start and the end of the answer in the text\n",
        "\n",
        "<font color='green'>Please use the *outputs* of the model to determine the most likely start and end of the answer span in your text, and then obtain the actual answer.  How satisfied are you with the answer?</font>  You may use help from the [ðŸ¤— Huggingface entry on question answering](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "id": "attended-style",
      "metadata": {
        "id": "attended-style",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afff354-2c19-4f51-b370-3fcacea7cdb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 Years old\n"
          ]
        }
      ],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "print(tokenizer.decode(predict_answer_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "destroyed-productivity",
      "metadata": {
        "id": "destroyed-productivity"
      },
      "source": [
        "<font color='green'>Please write a function called *answer_extraction* that gathers the previous operations: it takes two strings as arguments, creates instances of the tokenizer and the model, extracts the answer, and returns it as a string (possibly empty).  Do not create a new *tokenizer* and *model*, but assume that the ones you created above are global variables accessible from this function.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "helpful-companion",
      "metadata": {
        "id": "helpful-companion"
      },
      "outputs": [],
      "source": [
        "def answer_extraction(question: str, text: str) -> str:\n",
        "    inputs = tokenizer(question, text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    \n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    return tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "devoted-equilibrium",
      "metadata": {
        "id": "devoted-equilibrium"
      },
      "source": [
        "<font color='green'>Please test the function on the following questions and short text.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "encouraging-protein",
      "metadata": {
        "id": "encouraging-protein",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f5fe93-3694-4da5-bc43-603a6739b8e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I guess you are a very funny\n"
          ]
        }
      ],
      "source": [
        "print(answer_extraction('Are you funny?', 'Yes, I guess you are a very funny. I love it very much.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "completed-england",
      "metadata": {
        "id": "completed-england"
      },
      "source": [
        "## 2. Fragment retrieval using `Gensim` (from Lab 4)\n",
        "\n",
        "In this part, you will simply reuse code from Lab 4 to build a simple text retrieval system over the *Lee Corpus* provided with Gensim (300 news articles from the Australian Broadcasting Corporation).  \n",
        "* The [Gensim tutorial on topics and transformations](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py) provides the main idea.  \n",
        "* The goal is to retrieve, given a question, a short text fragment that is most likely to contain the answer.  As articles are not divided into paragraphs, you will refactor the collection of articles into a collection of fragments of at most *N* sentences each (without mixing articles). \n",
        "* The question will be used as a *query*, with the pre-processing options of your choice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions -q"
      ],
      "metadata": {
        "id": "JgBbf7DyjX50"
      },
      "id": "JgBbf7DyjX50",
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Modify path according to your configuration\n",
        "# !ls \"/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\"\n",
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/Colab Notebooks/MSE/AnTeDe/MSE_AnTeDe_Lab10_11')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSUcdE4MjFht",
        "outputId": "67ae19d9-df30-4793-b6f3-ffb5532f6849"
      },
      "id": "gSUcdE4MjFht",
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "forward-evaluation",
      "metadata": {
        "id": "forward-evaluation"
      },
      "outputs": [],
      "source": [
        "import gensim, nltk, os\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from TextPreprocessor import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "coupled-compression",
      "metadata": {
        "id": "coupled-compression"
      },
      "outputs": [],
      "source": [
        "N = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "insured-cable",
      "metadata": {
        "id": "insured-cable"
      },
      "source": [
        "<font color='green'>Load the articles of the Lee Background Corpus proviced with Gensim into a list of strings (each article in a string) called *raw_articles*.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "id": "quick-composite",
      "metadata": {
        "id": "quick-composite",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a678b5e8-9ea2-4d4a-d84e-a43f4b2d4830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
        "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
        "text = open(lee_train_file).read().splitlines()\n",
        "raw_articles = text\n",
        "print(len(raw_articles))\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "language = 'english'\n",
        "stop_words = set(stopwords.words(language))\n",
        "# Extend the list here:\n",
        "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
        "    stop_words.add(sw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intensive-operator",
      "metadata": {
        "id": "intensive-operator"
      },
      "source": [
        "<font color='green'>Please transform the articles into a collection of text fragments called *corpus1* (a list of lists of strings), by cutting each article into fragments of *N* consecutive sentences (e.g. *N* = 5), except possibly for the last fragment, and tokenizing each sentence.  At the end, display the number of fragments of your collection.</font>\n",
        "* Do not mix sentences from different articles in each fragment.\n",
        "* The reason for this operation is that full articles are too long to give to DistilBERT as texts. (Try it!)\n",
        "\n",
        "<font color='green'>Do not forget to pre-process the articles in preparation for search -- tokenization, stopword removal, and other operations if you want to explore them.</font>  \n",
        "* A  text fragment is thus a list of strings (tokens). \n",
        "* Please inspect your corpus to make sure it is correctly built.\n",
        "\n",
        "<font color='green'>If possible, store the original version of each fragment into a string in *corpus2* (i.e. non-tokenized, non-lowercased, etc.), because it will be better to pass it later to DistilBERT.  Otherwise, you can also reconstruct the full fragment from the tokens in corpus1.</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def split_seq(iterable, size):\n",
        "    it = iter(iterable)\n",
        "    item = list(itertools.islice(it, size))\n",
        "    while item:\n",
        "        yield ' '.join(item)\n",
        "        item = list(itertools.islice(it, size))"
      ],
      "metadata": {
        "id": "D5I4DrZDrVwG"
      },
      "id": "D5I4DrZDrVwG",
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "analyzed-ready",
      "metadata": {
        "id": "analyzed-ready",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a195284f-527d-4266-a160-ea1a07fa558c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fresh elections are not scheduled until March leaving whoever assumes the presidency with the daunting task of tackling Argentina's worst crisis in 12 years, but this time, isolated by international lending agencies.\n",
            "len of fragments\n",
            "790\n",
            "<class 'list'>\n",
            "300\n",
            "<class 'list'>\n",
            "4\n",
            "                                             corpus2  \\\n",
            "0  Hundreds of people have been forced to vacate ...   \n",
            "1  The New South Wales Rural Fire Service says th...   \n",
            "2  Rain has fallen in some parts of the Illawarra...   \n",
            "3  \"In fact, they've probably hampered the effort...   \n",
            "4  Indian security forces have shot dead eight su...   \n",
            "\n",
            "                                              corpus  \n",
            "0  hundred people force vacate home southern high...  \n",
            "1  new south wale rural fire service say weather ...  \n",
            "2  rain fall part illawarra sydney hunter valley ...  \n",
            "3  fact probably hamper effort firefighter wind g...  \n",
            "4  indian security force shot dead eight suspect ...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences_of_article = [list(split_seq(sent_tokenize(text), N)) for text in raw_articles]\n",
        "print((sentences_of_article[3][1]))\n",
        "corpus2 = sentences_of_article.copy()\n",
        "\n",
        "fragments = []\n",
        "[fragments.append(fragment) for frags in sentences_of_article for fragment in frags]\n",
        "print('len of fragments')\n",
        "print(len(fragments))\n",
        "\n",
        "fragments2 = fragments.copy()\n",
        "# TextPreprocessor? - get help regarding the attributes\n",
        "\n",
        "processor = TextPreprocessor(\n",
        "# Add options here:\n",
        " language = language,\n",
        " stopwords = stop_words\n",
        ")\n",
        "\n",
        "print(type(sentences_of_article))\n",
        "print(len(sentences_of_article))\n",
        "print(type(sentences_of_article[0]))\n",
        "print(len(sentences_of_article[0]))\n",
        "\n",
        "frags = pd.DataFrame(fragments, columns=['corpus2'])\n",
        "\n",
        "frags['corpus'] = processor.transform(frags['corpus2'])\n",
        "\n",
        "print(frags.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ethical-diabetes",
      "metadata": {
        "id": "ethical-diabetes"
      },
      "source": [
        "<font color='green'>Please create a search index (called *search_index*) using a *tfidf* model and transform all text fragments from *corpus1* into document vectors.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "charged-pastor",
      "metadata": {
        "id": "charged-pastor"
      },
      "outputs": [],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from gensim import models, corpora, similarities\n",
        "\n",
        "doc_tokenized = [simple_preprocess(doc) for doc in frags['corpus']]\n",
        "dictionary = corpora.Dictionary(doc_tokenized)\n",
        "corpus = [dictionary.doc2bow(text) for text in doc_tokenized]\n",
        "\n",
        "tfidf_model = models.TfidfModel(corpus)\n",
        "frags['search_index'] = [dictionary.doc2bow(doc.lower().split()) for doc in frags['corpus']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tough-architect",
      "metadata": {
        "id": "tough-architect"
      },
      "source": [
        "<font color='green'>Please write a function called *fragment_retrieval* which returns the most relevant text fragment (string) from the corpus given a question, which is used as the query.</font>  \n",
        "* The function processes the query in the same way as the documents (using the *tfidf model*) to obtain a *vectorized_query*.\n",
        "* This is passed to the *search_index* to rank all documents by relevance.\n",
        "* All the resources created above are supposed available as global variables (the dictionary, the tfidf model, the search_index, the corpus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "excessive-copper",
      "metadata": {
        "id": "excessive-copper"
      },
      "outputs": [],
      "source": [
        "from gensim.similarities import MatrixSimilarity\n",
        "def fragment_retrieval(query):\n",
        "    query = prepr\n",
        "    vec_bow = dictionary.doc2bow(query.lower().split())\n",
        "    sims = tfidf_model[vec_bow]\n",
        "    index = MatrixSimilarity(tfidf_model[corpus], num_best=1)\n",
        "    if len(index[sims]) < 1:\n",
        "        return ''\n",
        "    sims = index[sims][0]\n",
        "    return frags['corpus2'].iloc[sims[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parallel-science",
      "metadata": {
        "id": "parallel-science"
      },
      "source": [
        "<font color='green'>Please apply the above function to the three queries provided below.</font>  \n",
        "\n",
        "Note: again, the corpus, search_index, tfidf and dictionary are available as global variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "aware-worst",
      "metadata": {
        "id": "aware-worst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d128d5-13e5-4bd9-c3e2-3237f2ab2736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the mayor of New York? -> \"I felt that my job as the mayor was to turn around the city, because I believed - rightly or wrongly - that we had one last chance to do that.\" Mr Giuliani, a Republican, has served two terms as New York City's Mayor since 1993. Term limits prevent him from seeking a third term in office, and he will be succeeded by billionaire media mogul Michael Bloomberg.\n",
            "Who is Nicole Kidman? -> In the United States, Australian actress Nicole Kidman has been nominated for two Golden Globe best actor awards for her roles in the Australian-made musical \"Moulin Rouge\", and in her new thriller \"The Others\". \"Moulin Rouge\" also is one of two pictures leading the Golden Globe nominations, with six possible awards. It is vying for best musical or comedy picture of 2002, best actress in a comedy or musical, best actor in the same category for Ewen McGregor, best director for Baz Luhrmann, best original score and best original song. The other film to pick up six nominations is the Ron Howard directed \"A Beautiful Mind\" starring Oscar winner Russell Crowe.\n",
            "How many Australians died in the 1999 Interlaken canyoning accident? -> Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n",
            "What is Kieren Perkins' sport? -> Elka Graham won the women's 300 metres freestyle which was contested by having three races over 100 metres. She says former champion Kieren Perkins was behind her victory. \"I swam with Kieren this morning and he gave me some awesome advice. He told me to close my eyes [in the last 25 metres] and to absolutely go for it,\" she said.\n"
          ]
        }
      ],
      "source": [
        "queries = [\"Who is the mayor of New York?\", \n",
        "           \"Who is Nicole Kidman?\", \n",
        "           \"How many Australians died in the 1999 Interlaken canyoning accident?\",\n",
        "            \"What is Kieren Perkins' sport?\"]\n",
        "for q in queries:\n",
        "    print(q, '->', fragment_retrieval(q))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "excess-thousand",
      "metadata": {
        "id": "excess-thousand"
      },
      "source": [
        "## 3. Integration, testing and discussion\n",
        "\n",
        "<font color='green'>Using the two functions 'fragment_retrieval' and 'answer_extraction' from parts 1 and 2, and assuming all models and data are available as global variables, please create a unique function which returns the answer (string) to a question (string).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "medieval-trunk",
      "metadata": {
        "id": "medieval-trunk"
      },
      "outputs": [],
      "source": [
        "def question_answering(question):\n",
        "    text = fragment_retrieval(question)\n",
        "    if text is None:\n",
        "        text = ''\n",
        "    return answer_extraction(question, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valued-quilt",
      "metadata": {
        "id": "valued-quilt"
      },
      "source": [
        "<font color='green'>Please add between 5 and 10 more questions to the following list.  You can add answerable and non-answerable questions (with respect to the corpus).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "weighted-ottawa",
      "metadata": {
        "id": "weighted-ottawa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00892bb7-345d-46c3-b734-bddce9e32cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the mayor of New York? -> Mr Giuliani\n",
            "Who is Nicole Kidman? -> Australian actress\n",
            "How many Australians died in the 1999 Interlaken canyoning accident? -> 14\n",
            "What caused the 1999 Interlaken canyoning accident? -> thunderstorm\n",
            "Which city is the capital of Australia? -> Port - au - Prince\n",
            "Who is the prime-minister of Israel? -> Israel\n",
            "What are the main Australian airlines? -> The spokesman\n",
            "What is Kieren Perkins' sport? -> swam\n",
            "Which is the funniest sport? -> \n",
            "Which is the most famous sport in australia? -> [CLS]\n",
            "Where is the famous opera in australia? -> Launceston and Melbourne\n",
            "When was the last war australia was involved? -> inauguration ceremony\n",
            "When was the second world war? -> the American gained the ascendancy in the second set\n",
            "Name me a famous american actor. -> Mr Rini\n"
          ]
        }
      ],
      "source": [
        "questions = [\"Who is the mayor of New York?\", \n",
        "            \"Who is Nicole Kidman?\", \n",
        "            \"How many Australians died in the 1999 Interlaken canyoning accident?\",\n",
        "            \"What caused the 1999 Interlaken canyoning accident?\",\n",
        "            \"Which city is the capital of Australia?\",\n",
        "            \"Who is the prime-minister of Israel?\",\n",
        "            \"What are the main Australian airlines?\",\n",
        "            \"What is Kieren Perkins' sport?\",\n",
        "             \"Which is the funniest sport?\",\n",
        "             \"Which is the most famous sport in australia?\",\n",
        "             \"Where is the famous opera in australia?\",\n",
        "             \"When was the last war australia was involved?\",\n",
        "             \"When was the second world war?\", \n",
        "             \"Name me a famous american actor.\"]\n",
        "for q in questions:\n",
        "    print(q, '->', question_answering(q))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "square-depression",
      "metadata": {
        "id": "square-depression"
      },
      "source": [
        "<font color='green'>Please discuss the correctness of the answers, give possible reasons for incorrect ones, and make suggestions for improvements.</font>\n",
        "\n",
        "Write your discussion here or in a cell below.\n",
        "\n",
        "When you have finished please clean and re-run one last time the notebook, from start to end, then submit it on Moodle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparable-proxy",
      "metadata": {
        "id": "comparable-proxy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_nlp",
      "language": "python",
      "name": "pytorch_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "MSE_AnTeDe_Lab11_BERT4QA_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
