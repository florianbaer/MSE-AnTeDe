{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "official-furniture",
      "metadata": {
        "id": "official-furniture"
      },
      "source": [
        "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
        "\n",
        "# AnTeDe Lab 11: Question Answering using BERT\n",
        "\n",
        "by Andrei Popescu-Belis (HES-SO)\n",
        "using the [ðŸ¤— Huggingface models](https://huggingface.co/models),\n",
        "an [article by Marius Borcan](https://programmerbackpack.com/bert-nlp-using-distilbert-to-build-a-question-answering-system/) and \n",
        "an [article by Ramsi Goutham](https://towardsdatascience.com/simple-and-fast-question-answering-system-using-huggingface-distilbert-single-batch-inference-bcf5a5749571)\n",
        "\n",
        "**Summary**\n",
        "The goal of this lab is to implement and test a simple question answering (QA) system over a set of articles.  The structure of the lab is as follows:\n",
        "1. Answer extraction from a text fragment -- in this part, you will use a pre-trained model named DistilBERT (a lighter version of BERT) which can extract the most likely answer to a given question from a text fragment (in English).\n",
        "2. Text retrieval given a question -- in this part, you will reuse code from Lab 4 (Search Engine) to design a paragraph retrieval system over the 300-article Lee corpus provided with `gensim`. \n",
        "3. Integration and testing -- in this part, you will put together the functions from the previous two parts, and test your system end-to-end by designing a test set of 10 questions.\n",
        "\n",
        "## Implemented by:\n",
        "- Adrian Willi (adrian.willi@hslu.ch)\n",
        "- Florian BÃ¤r (florian.baer@hslu.ch)\n",
        "\n",
        "<font color='green'>Please answer the questions in green within this notebook.  The expected answers are generally very short: 1-2 commands or 2-3 lines of explanations.  At the end, please submit the completed notebook under the corresponding homework on Moodle.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "material-appreciation",
      "metadata": {
        "id": "material-appreciation"
      },
      "source": [
        "## 1. Answer extraction using DistilBERT\n",
        "\n",
        "As you know, the BERT pre-trained model can be fine-tuned for question answering, by training it to provide the start and end word of an input text fragment which is most likely the answer to an input question.  You will use the ðŸ¤— Huggingface Python module called `transformers`, and later use a DistilBERT model also provided by ðŸ¤— Huggingface.\n",
        "\n",
        "### a. Install `pytorch` and `transformers`\n",
        "\n",
        "Use the instructions provided by [PyTorch](https://pytorch.org/get-started/locally/#start-locally) and by [Huggingface](https://github.com/huggingface/transformers#installation).  The use of `conda` is recommended."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers -q"
      ],
      "metadata": {
        "id": "5Lk0BqGaiOHD"
      },
      "id": "5Lk0BqGaiOHD",
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "environmental-shadow",
      "metadata": {
        "id": "environmental-shadow"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleared-geology",
      "metadata": {
        "id": "cleared-geology"
      },
      "source": [
        "<font color='green'>Please generate a random 2x2x2 tensor with Pytorch.  Please display whether the workstation you use has a GPU or not.</font><br/>\n",
        "(Note: a GPU is not required for this lab.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "secret-fever",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "secret-fever",
        "outputId": "4ac4ccf2-6fca-4154-c226-e89a0b1059f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.9698, 0.9681],\n",
            "         [0.5777, 0.8024]],\n",
            "\n",
            "        [[0.8582, 0.4073],\n",
            "         [0.4133, 0.9966]]])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.rand((2,2,2)))\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "resistant-multimedia",
      "metadata": {
        "id": "resistant-multimedia"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wicked-junior",
      "metadata": {
        "id": "wicked-junior"
      },
      "source": [
        "ðŸ¤— Huggingface provides a very large repository of Transformer-based models at https://huggingface.co/models.\n",
        "\n",
        "<font color='green'>Please use the search interface (in a browser) and find out *how many models containing the name 'distilbert' for Question Answering* are available.  If we exclude those submitted by individual users, how many models are there left?  Please paste below their name and version date, and the size of their 'pytorch_model.bin' file.</font>\n",
        "\n",
        "\n",
        "<font color='green'>By looking at their \"model cards\", which model has the highest performance on the SQuAD dev set?</font>  In what follows, we will use this model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Totally, there are 203 Models containing the name 'distilbert' for question answering. \n",
        "\n",
        "Excluding those submitted by users there are 2 models.\n",
        "\n",
        "- **distilbert-base-uncased-distilled-squad** with a size of 253 MB\n",
        "  - F1 score of 86.9\n",
        "- **distilbert-base-cased-distilled-squad** with a size of 249 MB\n",
        "  - F1 score of 87.1"
      ],
      "metadata": {
        "id": "uEz7xCDtkbsJ"
      },
      "id": "uEz7xCDtkbsJ"
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "id": "functioning-volume",
      "metadata": {
        "id": "functioning-volume"
      },
      "outputs": [],
      "source": [
        " model_name = 'distilbert-base-cased-distilled-squad'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-surveillance",
      "metadata": {
        "id": "saved-surveillance"
      },
      "source": [
        "### b. Tokenization of the input\n",
        "\n",
        "We will use here a tokenizer called `DistilBertTokenizer` to tokenize the question and the text fragment and transform the numbers into numerical indices.  The documentation for this tokenizer is included in the general documentation of DistilBERT models at: https://huggingface.co/transformers/model_doc/distilbert.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "muslim-police",
      "metadata": {
        "id": "muslim-police"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, AutoTokenizer\n",
        "# you could use the AutoTokenizer as well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-extreme",
      "metadata": {
        "id": "ordered-extreme"
      },
      "source": [
        "<font color='green'>Please create an instance of such a tokenizer \n",
        "using the pre-trained model named 'distilbert-base-cased'.  The command\n",
        "will download the necessary model the first time you use it.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "id": "united-bargain",
      "metadata": {
        "id": "united-bargain"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comprehensive-wichita",
      "metadata": {
        "id": "comprehensive-wichita"
      },
      "source": [
        "<font color='green'>What does this instance return if you **call** it with a sentence (a *string*) as an argument?  Please write the instruction below, and be sure you include the word 'Winterthur' in your sentence.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "id": "prompt-depression",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prompt-depression",
        "outputId": "377d4941-1918-440a-cd03-8a842baa11d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "{'input_ids': [[101, 146, 1821, 1177, 6276, 1112, 178, 1821, 5497, 170, 173, 19593, 2511, 1107, 4591, 1582, 2149, 102], [101, 146, 1177, 1112, 178, 1821, 5497, 3489, 1107, 4591, 1582, 2149, 102, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "#print(tokenizer.tokenize('I am so funny as i am eating a dÃ¶ner in Winterthur'))\n",
        "#print(tokenizer.tokenize('I so as i am eating a dÃ¶ner in Winterthur'))\n",
        "sentences = ['I am so funny as i am eating a dÃ¶ner in Winterthur',\n",
        "             'I so as i am eating fish in Winterthur']\n",
        "tokens = tokenizer(sentences, padding=True)\n",
        "print(tokenizer(sentences[0]).keys())\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "maritime-agriculture",
      "metadata": {
        "id": "maritime-agriculture"
      },
      "source": [
        "<font color='green'>Please explain in your own words the meaning of the two components of the output above.  For that, please use the [documentation of the class DistilBertTokenizer](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer), and be sure you read the documentation of its *superclasses* as well.  Under what superclass do you find the links to the [glossary entries](https://huggingface.co/transformers/glossary.html) that best explain the two components, and what are these entries?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer returns as seen in the documentation the input ids and the attention_mask. The input_ids of the tokens are the ids fed into the model. These ids are used to identify a token. The attention mask is used to verify the length of the input sequece. This makes it possible to scale the input size fo the text."
      ],
      "metadata": {
        "id": "pR1wgFQIBVdt"
      },
      "id": "pR1wgFQIBVdt"
    },
    {
      "cell_type": "markdown",
      "id": "separate-chocolate",
      "metadata": {
        "id": "separate-chocolate"
      },
      "source": [
        "<font color='green'>If you haven't explained above, please explain here the cause of the difference between the number of words of your sentence, and the number of tokens in the observed output.  Please display the tokens of the output. You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentence is not split into words, but instead splitted into subword tokens, which often are used to represent less common words.This approach is similar as it is done in fasttext."
      ],
      "metadata": {
        "id": "ID7pAx3QziEX"
      },
      "id": "ID7pAx3QziEX"
    },
    {
      "cell_type": "markdown",
      "id": "competent-preparation",
      "metadata": {
        "id": "competent-preparation"
      },
      "source": [
        "<font color='green'>How can you convert back the first part of the output to the original string?\n",
        "Please write and execute the command(s) below.  You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "id": "flying-courtesy",
      "metadata": {
        "id": "flying-courtesy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079bfeec-8ec9-447f-9fda-8517b657ca33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] I am so funny as i am eating a dÃ¶ner in Winterthur [SEP]\n",
            "[CLS] I so as i am eating fish in Winterthur [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokens.input_ids[0]))\n",
        "print(tokenizer.decode(tokens.input_ids[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nuclear-auditor",
      "metadata": {
        "id": "nuclear-auditor"
      },
      "source": [
        "### c. Generation of input in the desired form\n",
        "\n",
        "We need to generate input in the form expected by the `DistilBertForQuestionAnswering` class.  This means providing the question, the text from which the answer must be extracted, with the proper [CLS] and [SEP] tokens, and the attention masks.  Moreover, using DistilBERT requires that the lists of indices returned by the tokenizer are Pytorch tensors (see tokenizer's option `return_tensors`).\n",
        "\n",
        "<font color='green'>What is the correct way to call the tokenizer in order to obtain these results?  You can use the example provided at the end of the [DistilBertForQuestionAnswering](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) documentation.  <br/>Please define a *question* and a *text* string of your own, and store the result of the tokenizer in a variable called *input*.  <br/>   Please verify (by converting back to the result) that the input has the correct tokens.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "id": "noticed-firmware",
      "metadata": {
        "id": "noticed-firmware",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1697f6-c4cf-454a-c5ec-6a3579cc3b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] What is your age? [SEP] As seen in the internet, the age of you is 25 Years old, which is very old. [SEP]\n",
            "28\n"
          ]
        }
      ],
      "source": [
        "question = 'What is your age?'\n",
        "answer = 'As seen in the internet, the age of you is 25 Years old, which is very old.'\n",
        "inputs = tokenizer(question, answer, return_tensors='pt')\n",
        "print(tokenizer.decode(inputs.input_ids[0]))\n",
        "print(inputs.input_ids[0].shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hindu-ridge",
      "metadata": {
        "id": "hindu-ridge"
      },
      "source": [
        "### d. Execution of the model over the input question and text\n",
        "\n",
        "In this section, you will create an instance of the BERT neural network adapted to question answering.  The class is named `DistilBertForQuestionAnswering`.  The model itself (the weights) is the one that you found at the end of (1a) above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "id": "alien-consortium",
      "metadata": {
        "id": "alien-consortium"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sporting-pricing",
      "metadata": {
        "id": "sporting-pricing"
      },
      "source": [
        "<font color='green'>Please create an instance of the model here.</font>  The data will be downloaded the first time you create it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "id": "fatty-disorder",
      "metadata": {
        "id": "fatty-disorder"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "packed-introduction",
      "metadata": {
        "id": "packed-introduction"
      },
      "source": [
        "The results of applying the model to your question and text (i.e. extracting the answer) are obtained by calling the model with the correct inputs.  \n",
        "\n",
        "<font color='green'>Please use the inputs you obtained above and read the [documentation of the DistilBertForQuestionAnswering class](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) (under *forward*) to apply the model to your data.  Store the results in a variable called *outputs*.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "southeast-murder",
      "metadata": {
        "id": "southeast-murder"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-rhythm",
      "metadata": {
        "id": "saved-rhythm"
      },
      "source": [
        "<font color='green'>Where are the probability values for the position of the **start** of the answer in *outputs*?</font> \n",
        "- The value of the logit is 9.8292.\n",
        "\n",
        "<font color='green'>Are these actual probabilities or other type of coefficients?</font> \n",
        "- These are not probabilities. Else they would sum up to 1 or 100% - what they don't do. They are the logits before SoftMax was applied.\n",
        "\n",
        "<font color='green'>\n",
        "How many values are there, and is this coherent with your observations in (1b)</font> \n",
        "\n",
        "- There is for each token given into the system a value as a logit to return the start and the end token as a response for the given question which was passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "id": "robust-monkey",
      "metadata": {
        "id": "robust-monkey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8682e850-6b7d-4479-c3ba-eae9774f0268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9.8292)\n",
            "Length of start_logits is torch.Size([1, 28])\n",
            "Index is from 18\n",
            "Index is to 20\n",
            "tensor(8.7799)\n",
            "2\n",
            "25 Years old\n"
          ]
        }
      ],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "print(outputs.start_logits.max())\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "print(f'Length of start_logits is {(outputs.start_logits.shape)}')\n",
        "print(f'Index is from {outputs.start_logits.argmax()}')\n",
        "print(f'Index is to {outputs.end_logits.argmax()}')\n",
        "print(outputs.end_logits.max())\n",
        "print(len(outputs))\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "print(tokenizer.decode(predict_answer_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scenic-custom",
      "metadata": {
        "id": "scenic-custom"
      },
      "source": [
        "### e. Determination of the start and the end of the answer in the text\n",
        "\n",
        "<font color='green'>Please use the *outputs* of the model to determine the most likely start and end of the answer span in your text, and then obtain the actual answer.  How satisfied are you with the answer?</font>  You may use help from the [ðŸ¤— Huggingface entry on question answering](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "id": "attended-style",
      "metadata": {
        "id": "attended-style",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95ee51d-61f4-49d0-8177-94e4c955c51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 Years old\n"
          ]
        }
      ],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "print(tokenizer.decode(predict_answer_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "destroyed-productivity",
      "metadata": {
        "id": "destroyed-productivity"
      },
      "source": [
        "<font color='green'>Please write a function called *answer_extraction* that gathers the previous operations: it takes two strings as arguments, creates instances of the tokenizer and the model, extracts the answer, and returns it as a string (possibly empty).  Do not create a new *tokenizer* and *model*, but assume that the ones you created above are global variables accessible from this function.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "helpful-companion",
      "metadata": {
        "id": "helpful-companion"
      },
      "outputs": [],
      "source": [
        "def answer_extraction(question: str, text: str) -> str:\n",
        "    inputs = tokenizer(question, text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    \n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    return tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "devoted-equilibrium",
      "metadata": {
        "id": "devoted-equilibrium"
      },
      "source": [
        "<font color='green'>Please test the function on the following questions and short text.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "id": "encouraging-protein",
      "metadata": {
        "id": "encouraging-protein",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74e8207-2169-4c63-c28c-56ff71ed52e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I guess you are a very funny\n"
          ]
        }
      ],
      "source": [
        "print(answer_extraction('Are you funny?', 'Yes, I guess you are a very funny. I love it very much.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "completed-england",
      "metadata": {
        "id": "completed-england"
      },
      "source": [
        "## 2. Fragment retrieval using `Gensim` (from Lab 4)\n",
        "\n",
        "In this part, you will simply reuse code from Lab 4 to build a simple text retrieval system over the *Lee Corpus* provided with Gensim (300 news articles from the Australian Broadcasting Corporation).  \n",
        "* The [Gensim tutorial on topics and transformations](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py) provides the main idea.  \n",
        "* The goal is to retrieve, given a question, a short text fragment that is most likely to contain the answer.  As articles are not divided into paragraphs, you will refactor the collection of articles into a collection of fragments of at most *N* sentences each (without mixing articles). \n",
        "* The question will be used as a *query*, with the pre-processing options of your choice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions -q"
      ],
      "metadata": {
        "id": "JgBbf7DyjX50"
      },
      "id": "JgBbf7DyjX50",
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Modify path according to your configuration\n",
        "# !ls \"/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\"\n",
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/Colab Notebooks/MSE/AnTeDe/MSE_AnTeDe_Lab10_11')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSUcdE4MjFht",
        "outputId": "e6486476-cd3c-47d4-ceb7-8c56a13a2664"
      },
      "id": "gSUcdE4MjFht",
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "forward-evaluation",
      "metadata": {
        "id": "forward-evaluation"
      },
      "outputs": [],
      "source": [
        "import gensim, nltk, os\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from TextPreprocessor import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "coupled-compression",
      "metadata": {
        "id": "coupled-compression"
      },
      "outputs": [],
      "source": [
        "N = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "insured-cable",
      "metadata": {
        "id": "insured-cable"
      },
      "source": [
        "<font color='green'>Load the articles of the Lee Background Corpus proviced with Gensim into a list of strings (each article in a string) called *raw_articles*.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "id": "quick-composite",
      "metadata": {
        "id": "quick-composite",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e609a0a-6628-45e8-8d48-ec85f8348d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
        "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
        "text = open(lee_train_file).read().splitlines()\n",
        "raw_articles = text\n",
        "print(len(raw_articles))\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "language = 'english'\n",
        "stop_words = set(stopwords.words(language))\n",
        "# Extend the list here:\n",
        "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s', '-']:\n",
        "    stop_words.add(sw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intensive-operator",
      "metadata": {
        "id": "intensive-operator"
      },
      "source": [
        "<font color='green'>Please transform the articles into a collection of text fragments called *corpus1* (a list of lists of strings), by cutting each article into fragments of *N* consecutive sentences (e.g. *N* = 5), except possibly for the last fragment, and tokenizing each sentence.  At the end, display the number of fragments of your collection.</font>\n",
        "* Do not mix sentences from different articles in each fragment.\n",
        "* The reason for this operation is that full articles are too long to give to DistilBERT as texts. (Try it!)\n",
        "\n",
        "<font color='green'>Do not forget to pre-process the articles in preparation for search -- tokenization, stopword removal, and other operations if you want to explore them.</font>  \n",
        "* A  text fragment is thus a list of strings (tokens). \n",
        "* Please inspect your corpus to make sure it is correctly built.\n",
        "\n",
        "<font color='green'>If possible, store the original version of each fragment into a string in *corpus2* (i.e. non-tokenized, non-lowercased, etc.), because it will be better to pass it later to DistilBERT.  Otherwise, you can also reconstruct the full fragment from the tokens in corpus1.</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def split_seq(iterable, size):\n",
        "    it = iter(iterable)\n",
        "    item = list(itertools.islice(it, size))\n",
        "    while item:\n",
        "        yield ' '.join(item)\n",
        "        item = list(itertools.islice(it, size))"
      ],
      "metadata": {
        "id": "D5I4DrZDrVwG"
      },
      "id": "D5I4DrZDrVwG",
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "id": "analyzed-ready",
      "metadata": {
        "id": "analyzed-ready",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d40036-95a9-4cfd-9153-c69189abb97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of fragments: 790\n",
            "                                             corpus2  \\\n",
            "0  Hundreds of people have been forced to vacate ...   \n",
            "1  The New South Wales Rural Fire Service says th...   \n",
            "2  Rain has fallen in some parts of the Illawarra...   \n",
            "3  \"In fact, they've probably hampered the effort...   \n",
            "4  Indian security forces have shot dead eight su...   \n",
            "\n",
            "                                             corpus1  \n",
            "0  hundred people force vacate home southern high...  \n",
            "1  new south wale rural fire service say weather ...  \n",
            "2  rain fall part illawarra sydney hunter valley ...  \n",
            "3  fact probably hamper effort firefighter wind g...  \n",
            "4  indian security force shot dead eight suspect ...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences_of_article = [list(split_seq(sent_tokenize(text), N)) for text in raw_articles]\n",
        "corpus2 = sentences_of_article.copy()\n",
        "\n",
        "fragments = []\n",
        "[fragments.append(fragment) for frags in sentences_of_article for fragment in frags]\n",
        "print(f'len of fragments: {len(fragments)}')\n",
        "\n",
        "fragments2 = fragments.copy()\n",
        "# TextPreprocessor? - get help regarding the attributes\n",
        "\n",
        "processor = TextPreprocessor(\n",
        "# Add options here:\n",
        " language = language,\n",
        " stopwords = stop_words,\n",
        " replace_contractions = True\n",
        ")\n",
        "\n",
        "\n",
        "frags = pd.DataFrame(fragments, columns=['corpus2'])\n",
        "\n",
        "frags['corpus1'] = processor.transform(frags['corpus2'])\n",
        "\n",
        "print(frags.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ethical-diabetes",
      "metadata": {
        "id": "ethical-diabetes"
      },
      "source": [
        "<font color='green'>Please create a search index (called *search_index*) using a *tfidf* model and transform all text fragments from *corpus1* into document vectors.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "id": "charged-pastor",
      "metadata": {
        "id": "charged-pastor",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "47fdfee7-8135-4583-8462-c3584155fc47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             corpus2  \\\n",
              "0  Hundreds of people have been forced to vacate ...   \n",
              "1  The New South Wales Rural Fire Service says th...   \n",
              "2  Rain has fallen in some parts of the Illawarra...   \n",
              "3  \"In fact, they've probably hampered the effort...   \n",
              "4  Indian security forces have shot dead eight su...   \n",
              "\n",
              "                                             corpus1  \n",
              "0  hundred people force vacate home southern high...  \n",
              "1  new south wale rural fire service say weather ...  \n",
              "2  rain fall part illawarra sydney hunter valley ...  \n",
              "3  fact probably hamper effort firefighter wind g...  \n",
              "4  indian security force shot dead eight suspect ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c627e20-312e-4a88-9d9c-7cd6be11116f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus2</th>\n",
              "      <th>corpus1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hundreds of people have been forced to vacate ...</td>\n",
              "      <td>hundred people force vacate home southern high...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The New South Wales Rural Fire Service says th...</td>\n",
              "      <td>new south wale rural fire service say weather ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rain has fallen in some parts of the Illawarra...</td>\n",
              "      <td>rain fall part illawarra sydney hunter valley ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"In fact, they've probably hampered the effort...</td>\n",
              "      <td>fact probably hamper effort firefighter wind g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indian security forces have shot dead eight su...</td>\n",
              "      <td>indian security force shot dead eight suspect ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c627e20-312e-4a88-9d9c-7cd6be11116f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8c627e20-312e-4a88-9d9c-7cd6be11116f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8c627e20-312e-4a88-9d9c-7cd6be11116f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from gensim import models, corpora, similarities\n",
        "\n",
        "doc_tokenized = [simple_preprocess(doc) for doc in frags['corpus1']]\n",
        "dictionary = corpora.Dictionary(doc_tokenized)\n",
        "corpus = [dictionary.doc2bow(text) for text in doc_tokenized]\n",
        "\n",
        "tfidf_model = models.TfidfModel(corpus)\n",
        "search_index = MatrixSimilarity(tfidf_model[corpus], num_best=1)\n",
        "frags.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tough-architect",
      "metadata": {
        "id": "tough-architect"
      },
      "source": [
        "<font color='green'>Please write a function called *fragment_retrieval* which returns the most relevant text fragment (string) from the corpus given a question, which is used as the query.</font>  \n",
        "* The function processes the query in the same way as the documents (using the *tfidf model*) to obtain a *vectorized_query*.\n",
        "* This is passed to the *search_index* to rank all documents by relevance.\n",
        "* All the resources created above are supposed available as global variables (the dictionary, the tfidf model, the search_index, the corpus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "id": "excessive-copper",
      "metadata": {
        "id": "excessive-copper"
      },
      "outputs": [],
      "source": [
        "from gensim.similarities import MatrixSimilarity\n",
        "def fragment_retrieval(query):\n",
        "    query = processor.preprocess_text(query)\n",
        "    print(f'query: {query}')\n",
        "    vec_bow = dictionary.doc2bow(query.split())\n",
        "    vec = tfidf_model[vec_bow]\n",
        "    sims = search_index[vec]\n",
        "    \n",
        "    text = frags['corpus2'].loc[sims[0][0]]\n",
        "    print(f'found with sim {sims[0][1]}')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parallel-science",
      "metadata": {
        "id": "parallel-science"
      },
      "source": [
        "<font color='green'>Please apply the above function to the three queries provided below.</font>  \n",
        "\n",
        "Note: again, the corpus, search_index, tfidf and dictionary are available as global variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "id": "aware-worst",
      "metadata": {
        "id": "aware-worst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c731ec5d-2cd6-4554-b2af-992089ee41e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: mayor new york\n",
            "found with sim 0.3441062271595001\n",
            "Who is the mayor of New York? -> \"I felt that my job as the mayor was to turn around the city, because I believed - rightly or wrongly - that we had one last chance to do that.\" Mr Giuliani, a Republican, has served two terms as New York City's Mayor since 1993. Term limits prevent him from seeking a third term in office, and he will be succeeded by billionaire media mogul Michael Bloomberg.\n",
            "query: nicole kidman\n",
            "found with sim 0.1537218987941742\n",
            "Who is Nicole Kidman? -> In the United States, Australian actress Nicole Kidman has been nominated for two Golden Globe best actor awards for her roles in the Australian-made musical \"Moulin Rouge\", and in her new thriller \"The Others\". \"Moulin Rouge\" also is one of two pictures leading the Golden Globe nominations, with six possible awards. It is vying for best musical or comedy picture of 2002, best actress in a comedy or musical, best actor in the same category for Ewen McGregor, best director for Baz Luhrmann, best original score and best original song. The other film to pick up six nominations is the Ron Howard directed \"A Beautiful Mind\" starring Oscar winner Russell Crowe.\n",
            "query: many australian die interlaken canyoning accident\n",
            "found with sim 0.3500305414199829\n",
            "How many Australians died in the 1999 Interlaken canyoning accident? -> Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n",
            "query: kieren perkins sport\n",
            "found with sim 0.3923473656177521\n",
            "What is Kieren Perkins' sport? -> Elka Graham won the women's 300 metres freestyle which was contested by having three races over 100 metres. She says former champion Kieren Perkins was behind her victory. \"I swam with Kieren this morning and he gave me some awesome advice. He told me to close my eyes [in the last 25 metres] and to absolutely go for it,\" she said.\n",
            "query: prime-minister israel\n",
            "found with sim 0.396584153175354\n",
            "Who is the prime-minister of Israel? -> The Middle East peace process is under new pressure after an ultimatum from the United States special envoy Anthony Zinni. After two weeks of frustration, he has given Israel and the Palestinians a 48 hour deadline to make some progress or he would go back to Washington. His mission has been accompanied by an upsurge in suicide bombing attacks on Israel and a tough Israeli response. Earlier, Israel rejected a temporary ceasefire offer by four militant Palestinian groups to halt their attacks through to the end of Ramadan next week if Israel agreed to stop assassinating their members.\n"
          ]
        }
      ],
      "source": [
        "queries = [\"Who is the mayor of New York?\", \n",
        "           \"Who is Nicole Kidman?\", \n",
        "           \"How many Australians died in the 1999 Interlaken canyoning accident?\",\n",
        "            \"What is Kieren Perkins' sport?\",\n",
        "           \"Who is the prime-minister of Israel?\"]\n",
        "for q in queries:\n",
        "    print(q, '->', fragment_retrieval(q))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "excess-thousand",
      "metadata": {
        "id": "excess-thousand"
      },
      "source": [
        "## 3. Integration, testing and discussion\n",
        "\n",
        "<font color='green'>Using the two functions 'fragment_retrieval' and 'answer_extraction' from parts 1 and 2, and assuming all models and data are available as global variables, please create a unique function which returns the answer (string) to a question (string).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "id": "medieval-trunk",
      "metadata": {
        "id": "medieval-trunk"
      },
      "outputs": [],
      "source": [
        "def question_answering(question):\n",
        "    print('\\n')\n",
        "    text = fragment_retrieval(question)\n",
        "    print(f'Retrieved text: {text}')\n",
        "    if text is None:\n",
        "        text = ''\n",
        "    return answer_extraction(question, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valued-quilt",
      "metadata": {
        "id": "valued-quilt"
      },
      "source": [
        "<font color='green'>Please add between 5 and 10 more questions to the following list.  You can add answerable and non-answerable questions (with respect to the corpus).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "id": "weighted-ottawa",
      "metadata": {
        "id": "weighted-ottawa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83b9c38-e581-44da-900a-02c98036a7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "query: mayor new york\n",
            "found with sim 0.3441062271595001\n",
            "Retrieved text: \"I felt that my job as the mayor was to turn around the city, because I believed - rightly or wrongly - that we had one last chance to do that.\" Mr Giuliani, a Republican, has served two terms as New York City's Mayor since 1993. Term limits prevent him from seeking a third term in office, and he will be succeeded by billionaire media mogul Michael Bloomberg.\n",
            "Who is the mayor of New York? -> Mr Giuliani\n",
            "\n",
            "\n",
            "query: nicole kidman\n",
            "found with sim 0.1537218987941742\n",
            "Retrieved text: In the United States, Australian actress Nicole Kidman has been nominated for two Golden Globe best actor awards for her roles in the Australian-made musical \"Moulin Rouge\", and in her new thriller \"The Others\". \"Moulin Rouge\" also is one of two pictures leading the Golden Globe nominations, with six possible awards. It is vying for best musical or comedy picture of 2002, best actress in a comedy or musical, best actor in the same category for Ewen McGregor, best director for Baz Luhrmann, best original score and best original song. The other film to pick up six nominations is the Ron Howard directed \"A Beautiful Mind\" starring Oscar winner Russell Crowe.\n",
            "Who is Nicole Kidman? -> Australian actress\n",
            "\n",
            "\n",
            "query: many australian die interlaken canyoning accident\n",
            "found with sim 0.3500305414199829\n",
            "Retrieved text: Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n",
            "How many Australians died in the 1999 Interlaken canyoning accident? -> 14\n",
            "\n",
            "\n",
            "query: cause interlaken canyoning accident\n",
            "found with sim 0.35462307929992676\n",
            "Retrieved text: Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n",
            "What caused the 1999 Interlaken canyoning accident? -> thunderstorm\n",
            "\n",
            "\n",
            "query: city capital australia\n",
            "found with sim 0.17624659836292267\n",
            "Retrieved text: At least four people, including two policemen, have been killed during an attempted coup in Haiti overnight. Armed commandos had stormed the national palace in the Haitian capital after midnight, local time and seized control of radio communications equipment. The attackers, understood to be former members of the Haitian military, fired at security guards as they entered the palace - the official residence of President Jean Bertrand Aristide. But the President was at another home in the capital Port-au-Prince during the attack.\n",
            "Which city is the capital of Australia? -> Port - au - Prince\n",
            "\n",
            "\n",
            "query: capital australia\n",
            "found with sim 0.2096738964319229\n",
            "Retrieved text: At least four people, including two policemen, have been killed during an attempted coup in Haiti overnight. Armed commandos had stormed the national palace in the Haitian capital after midnight, local time and seized control of radio communications equipment. The attackers, understood to be former members of the Haitian military, fired at security guards as they entered the palace - the official residence of President Jean Bertrand Aristide. But the President was at another home in the capital Port-au-Prince during the attack.\n",
            "What is the capital of Australia? -> Port - au - Prince\n",
            "\n",
            "\n",
            "query: prime-minister israel\n",
            "found with sim 0.396584153175354\n",
            "Retrieved text: The Middle East peace process is under new pressure after an ultimatum from the United States special envoy Anthony Zinni. After two weeks of frustration, he has given Israel and the Palestinians a 48 hour deadline to make some progress or he would go back to Washington. His mission has been accompanied by an upsurge in suicide bombing attacks on Israel and a tough Israeli response. Earlier, Israel rejected a temporary ceasefire offer by four militant Palestinian groups to halt their attacks through to the end of Ramadan next week if Israel agreed to stop assassinating their members.\n",
            "Who is the prime-minister of Israel? -> Anthony Zinni\n",
            "\n",
            "\n",
            "query: main australian airline\n",
            "found with sim 0.31708255410194397\n",
            "Retrieved text: An initial fleet of four Boeing 767-300 aircraft will eventually be increased to 12 and services will be extended to every Australian mainland capital, including Perth and Darwin. Australian Airlines is currently negotiating with various unions on wages, conditions and work practices. Australian Airlines is expected to announce it will base its operations in Cairns, following yesterday's Queensland Cabinet approval of an incentive package for the airline. The new airline plans to cut costs by outsourcing some maintenance work and reducing the number of flight attendants.\n",
            "What are the main Australian airlines? -> Perth and Darwin\n",
            "\n",
            "\n",
            "query: kieren perkins sport\n",
            "found with sim 0.3923473656177521\n",
            "Retrieved text: Elka Graham won the women's 300 metres freestyle which was contested by having three races over 100 metres. She says former champion Kieren Perkins was behind her victory. \"I swam with Kieren this morning and he gave me some awesome advice. He told me to close my eyes [in the last 25 metres] and to absolutely go for it,\" she said.\n",
            "What is Kieren Perkins' sport? -> swam\n",
            "\n",
            "\n",
            "query: name mountain australia\n",
            "found with sim 0.2949032187461853\n",
            "Retrieved text: The person with the longest list of supporters is Mr Rini, who has given massive import duty remissions on beer and cigarettes to favoured importers despite the parlous state of the Government's finances. The editor of a British tabloid has defended the resumption of its name and shame campaign against paedophiles saying parents have the right to know of potential dangers in the community. The paper published the names of convicted paedophiles who had failed to submit their names to the sex offenders register. The News of the World created uproar last year when it published the names and addresses of child sex offenders with vigilante groups formed and in some cases attacking the wrong men.\n",
            "What is the name of a mountain in australia? -> Mr Rini\n",
            "\n",
            "\n",
            "query: famous sport australia\n",
            "found with sim 0.11555179953575134\n",
            "Retrieved text: The agreement will also mean that a further 200 hectares at Rosebury and Bellamack will be free for urban growth without native title concerns. The Larrakia have also withdrawn their claim from the Archer Sporting Complex at Palmerston.\n",
            "What is a famous sport in australia? -> Larrakia\n",
            "\n",
            "\n",
            "query: fight fire\n",
            "found with sim 0.3100859224796295\n",
            "Retrieved text: The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area.\n",
            "Who fights fire? -> New South Wales Rural Fire Service\n",
            "\n",
            "\n",
            "query: last war australia involve\n",
            "found with sim 0.18602214753627777\n",
            "Retrieved text: Doctor Harris says any constitutional change must involve all Australians. \"I think it is very important that the people of Australia be given the opporunity to choose or be consulted at every stage of the process.\"\n",
            "When was the last war australia was involved? -> \n",
            "\n",
            "\n",
            "query: fight gaza\n",
            "found with sim 0.22058436274528503\n",
            "Retrieved text: The Israeli army has killed three Palestinian militants who attacked one of its armoured vehicles in the northern Gaza Strip. The three Palestinians opened fire with rifles at the vehicle between the Jewish settlements of Alei Sinai and Nitzanit, on the northern edge of the Gaza Strip. They were killed by shell fire from a tank, the sources said. During the fire fight an Israeli army observation post called in tank fire which killed the three gunmen.\n",
            "Who fights in gaza? -> Israeli\n",
            "\n",
            "\n",
            "query: name famous american actor\n",
            "found with sim 0.1698365956544876\n",
            "Retrieved text: The person with the longest list of supporters is Mr Rini, who has given massive import duty remissions on beer and cigarettes to favoured importers despite the parlous state of the Government's finances. The editor of a British tabloid has defended the resumption of its name and shame campaign against paedophiles saying parents have the right to know of potential dangers in the community. The paper published the names of convicted paedophiles who had failed to submit their names to the sex offenders register. The News of the World created uproar last year when it published the names and addresses of child sex offenders with vigilante groups formed and in some cases attacking the wrong men.\n",
            "What is the name of a famous american actor. -> Mr Rini\n"
          ]
        }
      ],
      "source": [
        "questions = [\"Who is the mayor of New York?\", \n",
        "            \"Who is Nicole Kidman?\", \n",
        "            \"How many Australians died in the 1999 Interlaken canyoning accident?\",\n",
        "            \"What caused the 1999 Interlaken canyoning accident?\",\n",
        "            \"Which city is the capital of Australia?\",\n",
        "            \"What is the capital of Australia?\",\n",
        "            \"Who is the prime-minister of Israel?\",\n",
        "            \"What are the main Australian airlines?\",\n",
        "            \"What is Kieren Perkins' sport?\",\n",
        "             \"What is the name of a mountain in australia?\",\n",
        "             \"What is a famous sport in australia?\",\n",
        "             \"Who fights fire?\",\n",
        "             \"When was the last war australia was involved?\",\n",
        "             \"Who fights in gaza?\", \n",
        "             \"What is the name of a famous american actor.\"]\n",
        "for q in questions:\n",
        "    print(q, '->', question_answering(q))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "square-depression",
      "metadata": {
        "id": "square-depression"
      },
      "source": [
        "<font color='green'>Please discuss the correctness of the answers, give possible reasons for incorrect ones, and make suggestions for improvements.</font>\n",
        "\n",
        "Write your discussion here or in a cell below.\n",
        "\n",
        "When you have finished please clean and re-run one last time the notebook, from start to end, then submit it on Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the questions are asked regarding the given corpus, the answer is found pretty well. But I noticed, that if the answers, that are not given in the document base, the answer is pretty random and quite useless. So to improve the question answering system, first we need to import the base corpus by adding more text to the system. There would be a possibilty to add the whole wikipedia dataset. Futher the transformer should be able to handle even more text, then we dont have to split the documents into sub-documents and the fragement-retrieval would work even better."
      ],
      "metadata": {
        "id": "Yi_z9JJm2iLh"
      },
      "id": "Yi_z9JJm2iLh"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m7nNDxHl3Ae4"
      },
      "id": "m7nNDxHl3Ae4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_nlp",
      "language": "python",
      "name": "pytorch_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "MSE_AnTeDe_Lab11_BERT4QA_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}