{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngmtgO_KGvCo"
   },
   "source": [
    "# AnTeDe Lab 3: Sentiment Analysis - Part B\n",
    "\n",
    "\n",
    "## Session goal\n",
    "The goal of this session is to build a baseline solution for unsupervised sentiment analysis. We begin by importing a well-known corpus of IMDB movie reviews labeled as either positive or negative. Take a few minutes to familiarize yourself with the variables *corpus* and *labels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6565,
     "status": "ok",
     "timestamp": 1647093558624,
     "user": {
      "displayName": "Florian Bär",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03438539089054973879"
     },
     "user_tz": -60
    },
    "id": "bimyiOZAGvCr",
    "outputId": "3c2c2f51-1240-4f77-8417-c28521698d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2022.3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import random, nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "corpus = [' '.join(movie_reviews.words(fileid)) \\\n",
    "          for category in movie_reviews.categories() \\\n",
    "          for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "labels = [category \\\n",
    "          for category in movie_reviews.categories() \\\n",
    "          for fileid in movie_reviews.fileids(category)]\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9nnzuAEGvCt"
   },
   "source": [
    "We extract a list of **positive_words** and a list of **negative_words** from the txt files in the **Words** folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1647093572119,
     "user": {
      "displayName": "Florian Bär",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03438539089054973879"
     },
     "user_tz": -60
    },
    "id": "HbYTzTh9HOv5"
   },
   "outputs": [],
   "source": [
    "negative_words_url = 'https://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "positive_words_url = 'https://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "\n",
    "def download_from_url(url, filepath):\n",
    "  import requests\n",
    "  r = requests.get(url)\n",
    "  with open(filepath, 'wb') as f:\n",
    "      f.write(r.content)\n",
    "\n",
    "!mkdir Words\n",
    "download_from_url(positive_words_url, 'Words/positive-words.txt')\n",
    "download_from_url(negative_words_url, 'Words/negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1647093621760,
     "user": {
      "displayName": "Florian Bär",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03438539089054973879"
     },
     "user_tz": -60
    },
    "id": "HKjDXu5zGvCu"
   },
   "outputs": [],
   "source": [
    "with open('Words/positive-words.txt', errors='ignore') as opened:\n",
    "    contents=opened.read()\n",
    "contents_lines=['a+'] + contents.split('a+')[1].split('\\n')\n",
    "\n",
    "positive_words = [x for x in contents_lines if len(x)>0]\n",
    "\n",
    "\n",
    "with open('Words/negative-words.txt', errors='ignore') as opened:\n",
    "    contents=opened.read()\n",
    "contents_lines=['2-faced'] + contents.split('2-faced')[1].split('\\n')\n",
    "\n",
    "negative_words = [x for x in contents_lines if len(x)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jqkDOPUGvCu"
   },
   "source": [
    "We reuse our helper function **get_metrics** from Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1647093629540,
     "user": {
      "displayName": "Florian Bär",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03438539089054973879"
     },
     "user_tz": -60
    },
    "id": "9mPKYySgGvCv"
   },
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "        from sklearn import metrics\n",
    "        import numpy as np\n",
    "        print ('Accuracy:', np.round(\n",
    "            metrics.accuracy_score(true_labels,\n",
    "            predicted_labels), 3))\n",
    "        \n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNs11NF5GvCv"
   },
   "source": [
    "Your task: complete the function **baseline_sentiment_analysis** so that:\n",
    "- it counts the number of positive words and the number of negative words in each review in the corpus;\n",
    "- if the positive word count is greater than the negative word count, it labels the review as positive\n",
    "- if the positive word count is lower than or equal to the negative word count, it labels the review as negative\n",
    "\n",
    "When you've completed the function, run the cell to measure the performance of your baseline unsupervised sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10936,
     "status": "ok",
     "timestamp": 1647093675800,
     "user": {
      "displayName": "Florian Bär",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03438539089054973879"
     },
     "user_tz": -60
    },
    "id": "Sf2AV8rbGvCw",
    "outputId": "979ffa72-7b7c-42bd-e1be-4b7a1f125d9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.82      0.74      1000\n",
      "         pos       0.77      0.60      0.67      1000\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.72      0.71      0.71      2000\n",
      "weighted avg       0.72      0.71      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def baseline_sentiment_analysis(corpus):\n",
    "    \n",
    "    labels=[]\n",
    "    scores=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        assert(1==1)\n",
    "# BEGIN_REMOVE\n",
    "        document_words = set(word for word in word_tokenize(document))\n",
    "        positive_words_in_doc = len(list(document_words.intersection(positive_words)))\n",
    "        negative_words_in_doc = len(list(document_words.intersection(negative_words)))\n",
    "        score=positive_words_in_doc-negative_words_in_doc\n",
    "        if score>0:\n",
    "            labels.append('pos')\n",
    "        else:\n",
    "            labels.append('neg')\n",
    "        \n",
    "        scores.append(score)    \n",
    "# END_REMOVE        \n",
    "    return labels,  scores\n",
    "\n",
    "predicted_labels, scores = baseline_sentiment_analysis(corpus)\n",
    "\n",
    "try:\n",
    "    get_metrics(true_labels=labels,\n",
    "            predicted_labels=predicted_labels)\n",
    "except:\n",
    "    print ('The function baseline_sentiment_analysis needs your attention.')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ7mqgCZGvCx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MSE_AnTeDe_Lab3b_Unsupervised.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
